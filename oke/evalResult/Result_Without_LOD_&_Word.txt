test Maximum Entropy classifer for class induction
training data is loaded!
gazetteers are loaded!
skip computing features...
load features from 'trainWithFeatures.json'... 
train set size 2134
 split  70.0 % from gold standards for training ... 
start [1] fold validation...
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -0.69315        0.102
             2          -0.19366        0.898
             3          -0.18965        0.898
             4          -0.18531        0.898
             5          -0.18083        0.898
             6          -0.17637        0.898
             7          -0.17201        0.898
             8          -0.16783        0.898
             9          -0.16385        0.898
            10          -0.16009        0.899
            11          -0.15654        0.902
            12          -0.15319        0.906
            13          -0.15003        0.907
            14          -0.14705        0.914
            15          -0.14424        0.919
            16          -0.14157        0.921
            17          -0.13904        0.924
            18          -0.13663        0.928
            19          -0.13434        0.931
            20          -0.13216        0.934
            21          -0.13007        0.934
            22          -0.12807        0.935
            23          -0.12615        0.938
            24          -0.12431        0.940
            25          -0.12254        0.943
            26          -0.12084        0.944
            27          -0.11920        0.945
            28          -0.11762        0.946
            29          -0.11609        0.945
            30          -0.11461        0.945
            31          -0.11319        0.947
            32          -0.11181        0.948
            33          -0.11047        0.949
            34          -0.10917        0.948
            35          -0.10791        0.949
            36          -0.10669        0.951
            37          -0.10551        0.952
            38          -0.10435        0.953
            39          -0.10323        0.953
            40          -0.10214        0.954
            41          -0.10108        0.955
            42          -0.10005        0.956
            43          -0.09904        0.957
            44          -0.09806        0.958
            45          -0.09711        0.958
            46          -0.09617        0.959
            47          -0.09526        0.960
            48          -0.09437        0.961
            49          -0.09351        0.962
            50          -0.09266        0.962
            51          -0.09183        0.963
            52          -0.09102        0.963
            53          -0.09022        0.963
            54          -0.08945        0.964
            55          -0.08869        0.964
            56          -0.08794        0.964
            57          -0.08722        0.964
            58          -0.08650        0.965
            59          -0.08580        0.965
            60          -0.08512        0.967
            61          -0.08445        0.967
            62          -0.08379        0.967
            63          -0.08314        0.967
            64          -0.08251        0.967
            65          -0.08189        0.967
            66          -0.08128        0.968
            67          -0.08068        0.968
            68          -0.08009        0.969
            69          -0.07951        0.970
            70          -0.07894        0.972
            71          -0.07838        0.972
            72          -0.07783        0.972
            73          -0.07729        0.972
            74          -0.07676        0.973
            75          -0.07624        0.973
            76          -0.07573        0.973
            77          -0.07522        0.974
            78          -0.07473        0.974
            79          -0.07424        0.975
            80          -0.07375        0.976
            81          -0.07328        0.977
            82          -0.07281        0.977
            83          -0.07235        0.977
            84          -0.07190        0.977
            85          -0.07145        0.977
            86          -0.07101        0.979
            87          -0.07058        0.979
            88          -0.07015        0.979
            89          -0.06973        0.979
            90          -0.06932        0.979
            91          -0.06891        0.980
            92          -0.06851        0.980
            93          -0.06811        0.980
            94          -0.06771        0.980
            95          -0.06733        0.980
            96          -0.06695        0.981
            97          -0.06657        0.981
            98          -0.06620        0.981
            99          -0.06583        0.981
         Final          -0.06547        0.981
accuracy: 0.9359375
precision: 0.7222222222222222
recall: 0.45614035087719296
F-measure: 0.5591397849462365
========Show top 10 most informative features========
   2.074 prev_5_word_last_2_letters=='81' and label is 'class'
   1.810 next_1_word_last_2_letters=='da' and label is 'class'
  -1.776 last_2_letters=='None' and label is 'class'
   1.618 prev_7_word_last_2_letters=='to' and label is 'class'
   1.608 prev_5_word_last_2_letters=='ep' and label is 'class'
   1.608 prev_3_word_last_2_letters=='it' and label is 'class'
   1.582 prev_3_word_last_2_letters=='ue' and label is 'class'
   1.492 prev_1_word_last_2_letters=='ft' and label is 'class'
  -1.379 word_pos=='IN' and label is 'class'
   1.343 prev_7_word_last_2_letters=='81' and label is 'class'
start [2] fold validation...
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -0.69315        0.100
             2          -0.18960        0.900
             3          -0.18585        0.900
             4          -0.18170        0.900
             5          -0.17739        0.900
             6          -0.17307        0.900
             7          -0.16885        0.900
             8          -0.16478        0.900
             9          -0.16090        0.900
            10          -0.15722        0.900
            11          -0.15374        0.902
            12          -0.15044        0.906
            13          -0.14733        0.910
            14          -0.14438        0.914
            15          -0.14158        0.918
            16          -0.13893        0.922
            17          -0.13641        0.928
            18          -0.13400        0.929
            19          -0.13171        0.930
            20          -0.12952        0.932
            21          -0.12742        0.936
            22          -0.12542        0.938
            23          -0.12349        0.940
            24          -0.12165        0.942
            25          -0.11987        0.944
            26          -0.11816        0.945
            27          -0.11652        0.946
            28          -0.11493        0.947
            29          -0.11340        0.948
            30          -0.11193        0.950
            31          -0.11050        0.950
            32          -0.10912        0.951
            33          -0.10779        0.951
            34          -0.10649        0.952
            35          -0.10524        0.952
            36          -0.10403        0.954
            37          -0.10285        0.954
            38          -0.10171        0.956
            39          -0.10060        0.956
            40          -0.09952        0.957
            41          -0.09847        0.957
            42          -0.09745        0.958
            43          -0.09646        0.959
            44          -0.09549        0.961
            45          -0.09455        0.961
            46          -0.09363        0.962
            47          -0.09274        0.962
            48          -0.09186        0.962
            49          -0.09101        0.963
            50          -0.09018        0.963
            51          -0.08937        0.963
            52          -0.08858        0.965
            53          -0.08780        0.965
            54          -0.08705        0.967
            55          -0.08631        0.969
            56          -0.08558        0.970
            57          -0.08487        0.971
            58          -0.08418        0.971
            59          -0.08350        0.972
            60          -0.08284        0.973
            61          -0.08219        0.973
            62          -0.08155        0.974
            63          -0.08092        0.975
            64          -0.08031        0.975
            65          -0.07971        0.975
            66          -0.07912        0.975
            67          -0.07854        0.975
            68          -0.07797        0.975
            69          -0.07741        0.976
            70          -0.07686        0.977
            71          -0.07633        0.977
            72          -0.07580        0.977
            73          -0.07528        0.977
            74          -0.07477        0.977
            75          -0.07426        0.977
            76          -0.07377        0.977
            77          -0.07329        0.977
            78          -0.07281        0.977
            79          -0.07234        0.977
            80          -0.07188        0.977
            81          -0.07142        0.978
            82          -0.07097        0.978
            83          -0.07053        0.978
            84          -0.07010        0.979
            85          -0.06967        0.979
            86          -0.06925        0.979
            87          -0.06883        0.979
            88          -0.06842        0.979
            89          -0.06802        0.979
            90          -0.06762        0.979
            91          -0.06723        0.980
            92          -0.06685        0.980
            93          -0.06646        0.981
            94          -0.06609        0.981
            95          -0.06572        0.981
            96          -0.06535        0.981
            97          -0.06499        0.981
            98          -0.06463        0.981
            99          -0.06428        0.981
         Final          -0.06393        0.981
accuracy: 0.9359375
precision: 0.7567567567567568
recall: 0.4666666666666667
F-measure: 0.5773195876288659
========Show top 10 most informative features========
   2.360 prev_6_word_last_2_letters=='81' and label is 'class'
  -2.151 last_2_letters=='None' and label is 'class'
   2.080 prev_5_word_last_2_letters=='81' and label is 'class'
   1.747 next_1_word_last_2_letters=='da' and label is 'class'
   1.608 prev_5_word_last_2_letters=='ep' and label is 'class'
   1.550 prev_7_word_last_2_letters=='to' and label is 'class'
  -1.497 prev_1_word_pos=='IN' and label is 'class'
   1.442 prev_3_word_last_2_letters=='ue' and label is 'class'
   1.378 last_2_letters=='la' and label is 'class'
   1.374 last_2_letters=='ft' and label is 'class'
start [3] fold validation...
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -0.69315        0.100
             2          -0.19014        0.900
             3          -0.18640        0.900
             4          -0.18225        0.900
             5          -0.17794        0.900
             6          -0.17360        0.900
             7          -0.16935        0.900
             8          -0.16524        0.900
             9          -0.16132        0.900
            10          -0.15760        0.902
            11          -0.15409        0.904
            12          -0.15077        0.910
            13          -0.14764        0.912
            14          -0.14469        0.916
            15          -0.14189        0.919
            16          -0.13925        0.923
            17          -0.13674        0.924
            18          -0.13436        0.927
            19          -0.13209        0.930
            20          -0.12993        0.932
            21          -0.12787        0.934
            22          -0.12590        0.934
            23          -0.12401        0.935
            24          -0.12219        0.938
            25          -0.12045        0.940
            26          -0.11878        0.941
            27          -0.11717        0.942
            28          -0.11562        0.943
            29          -0.11412        0.944
            30          -0.11268        0.945
            31          -0.11128        0.948
            32          -0.10993        0.948
            33          -0.10863        0.950
            34          -0.10736        0.950
            35          -0.10613        0.954
            36          -0.10494        0.954
            37          -0.10379        0.954
            38          -0.10267        0.954
            39          -0.10158        0.955
            40          -0.10052        0.956
            41          -0.09949        0.958
            42          -0.09848        0.958
            43          -0.09751        0.959
            44          -0.09656        0.960
            45          -0.09563        0.960
            46          -0.09472        0.961
            47          -0.09384        0.961
            48          -0.09298        0.961
            49          -0.09214        0.962
            50          -0.09132        0.962
            51          -0.09051        0.964
            52          -0.08973        0.965
            53          -0.08896        0.966
            54          -0.08821        0.966
            55          -0.08748        0.966
            56          -0.08676        0.967
            57          -0.08605        0.967
            58          -0.08536        0.968
            59          -0.08469        0.969
            60          -0.08402        0.969
            61          -0.08338        0.971
            62          -0.08274        0.971
            63          -0.08211        0.971
            64          -0.08150        0.972
            65          -0.08090        0.973
            66          -0.08031        0.973
            67          -0.07973        0.973
            68          -0.07916        0.974
            69          -0.07860        0.974
            70          -0.07805        0.974
            71          -0.07751        0.974
            72          -0.07698        0.974
            73          -0.07646        0.974
            74          -0.07594        0.974
            75          -0.07544        0.975
            76          -0.07494        0.975
            77          -0.07445        0.976
            78          -0.07397        0.976
            79          -0.07350        0.976
            80          -0.07303        0.977
            81          -0.07257        0.977
            82          -0.07212        0.978
            83          -0.07167        0.978
            84          -0.07123        0.978
            85          -0.07080        0.978
            86          -0.07037        0.979
            87          -0.06995        0.979
            88          -0.06954        0.979
            89          -0.06913        0.979
            90          -0.06873        0.980
            91          -0.06833        0.980
            92          -0.06794        0.981
            93          -0.06755        0.981
            94          -0.06717        0.981
            95          -0.06679        0.981
            96          -0.06642        0.981
            97          -0.06605        0.981
            98          -0.06569        0.981
            99          -0.06533        0.981
         Final          -0.06498        0.981
accuracy: 0.9359375
precision: 0.8064516129032258
recall: 0.4166666666666667
F-measure: 0.5494505494505495
========Show top 10 most informative features========
   2.065 prev_5_word_last_2_letters=='81' and label is 'class'
  -1.749 last_2_letters=='None' and label is 'class'
   1.722 next_1_word_last_2_letters=='da' and label is 'class'
   1.442 prev_2_word_last_2_letters=='ue' and label is 'class'
   1.423 prev_1_word_last_2_letters=='11' and label is 'class'
  -1.408 word_pos=='IN' and label is 'class'
   1.392 prev_7_word_last_2_letters=='to' and label is 'class'
   1.382 prev_4_word_last_2_letters=='HD' and label is 'class'
   1.300 next_1_word_last_2_letters=='x1' and label is 'class'
   1.295 prev_7_word_last_2_letters=='81' and label is 'class'
start [4] fold validation...
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -0.69315        0.098
             2          -0.18753        0.902
             3          -0.18387        0.902
             4          -0.17983        0.902
             5          -0.17561        0.902
             6          -0.17136        0.902
             7          -0.16717        0.902
             8          -0.16310        0.902
             9          -0.15920        0.902
            10          -0.15547        0.902
            11          -0.15194        0.903
            12          -0.14859        0.906
            13          -0.14542        0.910
            14          -0.14241        0.912
            15          -0.13956        0.918
            16          -0.13686        0.922
            17          -0.13429        0.927
            18          -0.13184        0.929
            19          -0.12950        0.930
            20          -0.12726        0.933
            21          -0.12513        0.934
            22          -0.12308        0.934
            23          -0.12112        0.936
            24          -0.11923        0.937
            25          -0.11742        0.939
            26          -0.11567        0.941
            27          -0.11399        0.942
            28          -0.11237        0.942
            29          -0.11080        0.944
            30          -0.10929        0.946
            31          -0.10783        0.948
            32          -0.10641        0.948
            33          -0.10505        0.950
            34          -0.10372        0.952
            35          -0.10243        0.952
            36          -0.10118        0.952
            37          -0.09997        0.952
            38          -0.09880        0.956
            39          -0.09765        0.958
            40          -0.09654        0.959
            41          -0.09546        0.959
            42          -0.09441        0.962
            43          -0.09339        0.962
            44          -0.09239        0.963
            45          -0.09141        0.965
            46          -0.09047        0.966
            47          -0.08954        0.966
            48          -0.08864        0.967
            49          -0.08776        0.967
            50          -0.08690        0.969
            51          -0.08605        0.969
            52          -0.08523        0.969
            53          -0.08443        0.969
            54          -0.08364        0.971
            55          -0.08288        0.973
            56          -0.08212        0.973
            57          -0.08139        0.973
            58          -0.08067        0.973
            59          -0.07996        0.975
            60          -0.07927        0.975
            61          -0.07859        0.975
            62          -0.07792        0.975
            63          -0.07727        0.975
            64          -0.07663        0.975
            65          -0.07601        0.975
            66          -0.07539        0.975
            67          -0.07479        0.975
            68          -0.07419        0.975
            69          -0.07361        0.976
            70          -0.07304        0.977
            71          -0.07248        0.977
            72          -0.07192        0.978
            73          -0.07138        0.978
            74          -0.07085        0.978
            75          -0.07032        0.979
            76          -0.06981        0.980
            77          -0.06930        0.981
            78          -0.06880        0.981
            79          -0.06831        0.981
            80          -0.06782        0.981
            81          -0.06735        0.981
            82          -0.06688        0.981
            83          -0.06642        0.982
            84          -0.06596        0.983
            85          -0.06552        0.983
            86          -0.06508        0.984
            87          -0.06464        0.984
            88          -0.06421        0.984
            89          -0.06379        0.984
            90          -0.06338        0.984
            91          -0.06297        0.984
            92          -0.06256        0.985
            93          -0.06216        0.985
            94          -0.06177        0.985
            95          -0.06138        0.985
            96          -0.06100        0.985
            97          -0.06063        0.985
            98          -0.06025        0.987
            99          -0.05989        0.987
         Final          -0.05952        0.987
accuracy: 0.946875
precision: 0.8918918918918919
recall: 0.5238095238095238
F-measure: 0.66
========Show top 10 most informative features========
  -2.295 last_2_letters=='None' and label is 'class'
   2.010 prev_1_word_last_2_letters=='11' and label is 'class'
   1.938 prev_6_word_last_2_letters=='81' and label is 'class'
   1.665 next_1_word_last_2_letters=='x1' and label is 'class'
   1.572 prev_5_word_last_2_letters=='ep' and label is 'class'
   1.350 prev_2_word_last_2_letters=='ue' and label is 'class'
   1.295 prev_3_word_last_2_letters=='ue' and label is 'class'
  -1.291 next_1_word_pos=='DT' and label is 'class'
  -1.267 word_pos=='DT' and label is 'class'
   1.220 next_1_word_word_with_digits=='Y' and label is 'class'
start [5] fold validation...
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -0.69315        0.098
             2          -0.18653        0.902
             3          -0.18318        0.902
             4          -0.17946        0.902
             5          -0.17558        0.902
             6          -0.17167        0.902
             7          -0.16781        0.902
             8          -0.16406        0.902
             9          -0.16046        0.902
            10          -0.15701        0.902
            11          -0.15373        0.903
            12          -0.15061        0.904
            13          -0.14764        0.906
            14          -0.14481        0.912
            15          -0.14212        0.915
            16          -0.13956        0.918
            17          -0.13711        0.923
            18          -0.13477        0.926
            19          -0.13253        0.929
            20          -0.13039        0.932
            21          -0.12833        0.933
            22          -0.12635        0.934
            23          -0.12445        0.934
            24          -0.12262        0.935
            25          -0.12086        0.938
            26          -0.11916        0.940
            27          -0.11752        0.942
            28          -0.11594        0.944
            29          -0.11441        0.945
            30          -0.11293        0.946
            31          -0.11149        0.946
            32          -0.11010        0.948
            33          -0.10876        0.948
            34          -0.10745        0.949
            35          -0.10618        0.948
            36          -0.10495        0.950
            37          -0.10376        0.950
            38          -0.10259        0.952
            39          -0.10146        0.954
            40          -0.10036        0.957
            41          -0.09930        0.959
            42          -0.09825        0.959
            43          -0.09724        0.962
            44          -0.09625        0.962
            45          -0.09529        0.963
            46          -0.09435        0.966
            47          -0.09343        0.967
            48          -0.09254        0.967
            49          -0.09166        0.967
            50          -0.09081        0.968
            51          -0.08998        0.968
            52          -0.08916        0.968
            53          -0.08837        0.968
            54          -0.08759        0.969
            55          -0.08683        0.970
            56          -0.08608        0.970
            57          -0.08535        0.971
            58          -0.08464        0.971
            59          -0.08394        0.971
            60          -0.08325        0.972
            61          -0.08258        0.972
            62          -0.08193        0.972
            63          -0.08128        0.972
            64          -0.08065        0.972
            65          -0.08003        0.972
            66          -0.07942        0.972
            67          -0.07883        0.972
            68          -0.07824        0.972
            69          -0.07767        0.973
            70          -0.07710        0.973
            71          -0.07655        0.973
            72          -0.07601        0.973
            73          -0.07547        0.973
            74          -0.07495        0.973
            75          -0.07443        0.974
            76          -0.07392        0.974
            77          -0.07342        0.975
            78          -0.07293        0.975
            79          -0.07245        0.975
            80          -0.07198        0.975
            81          -0.07151        0.976
            82          -0.07105        0.977
            83          -0.07060        0.977
            84          -0.07015        0.977
            85          -0.06971        0.979
            86          -0.06928        0.980
            87          -0.06885        0.980
            88          -0.06843        0.980
            89          -0.06802        0.981
            90          -0.06761        0.981
            91          -0.06721        0.981
            92          -0.06682        0.981
            93          -0.06643        0.981
            94          -0.06604        0.982
            95          -0.06566        0.982
            96          -0.06529        0.982
            97          -0.06492        0.982
            98          -0.06456        0.982
            99          -0.06420        0.982
         Final          -0.06384        0.982
accuracy: 0.9328125
precision: 0.7380952380952381
recall: 0.49206349206349204
F-measure: 0.5904761904761905
========Show top 10 most informative features========
   2.427 prev_5_word_last_2_letters=='81' and label is 'class'
   2.151 prev_6_word_last_2_letters=='81' and label is 'class'
   1.972 next_1_word_last_2_letters=='da' and label is 'class'
  -1.716 last_2_letters=='None' and label is 'class'
   1.449 prev_5_word_last_2_letters=='ep' and label is 'class'
   1.449 prev_3_word_last_2_letters=='it' and label is 'class'
  -1.448 word_pos=='DT' and label is 'class'
  -1.414 word_pos=='IN' and label is 'class'
   1.298 prev_2_word_last_2_letters=='ue' and label is 'class'
  -1.210 next_1_word_pos=='DT' and label is 'class'
start [6] fold validation...
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -0.69315        0.098
             2          -0.18671        0.902
             3          -0.18350        0.902
             4          -0.17995        0.902
             5          -0.17626        0.902
             6          -0.17253        0.902
             7          -0.16885        0.902
             8          -0.16526        0.902
             9          -0.16178        0.902
            10          -0.15843        0.902
            11          -0.15522        0.902
            12          -0.15214        0.902
            13          -0.14919        0.903
            14          -0.14637        0.910
            15          -0.14366        0.912
            16          -0.14107        0.914
            17          -0.13858        0.915
            18          -0.13618        0.918
            19          -0.13388        0.921
            20          -0.13167        0.923
            21          -0.12954        0.925
            22          -0.12749        0.927
            23          -0.12551        0.930
            24          -0.12361        0.931
            25          -0.12176        0.933
            26          -0.11999        0.936
            27          -0.11827        0.942
            28          -0.11661        0.943
            29          -0.11500        0.944
            30          -0.11344        0.945
            31          -0.11193        0.946
            32          -0.11047        0.948
            33          -0.10906        0.948
            34          -0.10768        0.950
            35          -0.10635        0.950
            36          -0.10506        0.950
            37          -0.10380        0.951
            38          -0.10258        0.951
            39          -0.10139        0.953
            40          -0.10023        0.956
            41          -0.09911        0.958
            42          -0.09802        0.959
            43          -0.09695        0.961
            44          -0.09591        0.961
            45          -0.09490        0.961
            46          -0.09392        0.962
            47          -0.09296        0.963
            48          -0.09202        0.964
            49          -0.09111        0.966
            50          -0.09021        0.969
            51          -0.08934        0.972
            52          -0.08849        0.972
            53          -0.08766        0.972
            54          -0.08684        0.972
            55          -0.08605        0.972
            56          -0.08527        0.972
            57          -0.08451        0.973
            58          -0.08376        0.973
            59          -0.08304        0.973
            60          -0.08232        0.975
            61          -0.08162        0.975
            62          -0.08094        0.976
            63          -0.08027        0.977
            64          -0.07961        0.977
            65          -0.07896        0.977
            66          -0.07833        0.977
            67          -0.07771        0.977
            68          -0.07710        0.978
            69          -0.07650        0.978
            70          -0.07592        0.978
            71          -0.07534        0.979
            72          -0.07478        0.979
            73          -0.07422        0.979
            74          -0.07368        0.979
            75          -0.07314        0.979
            76          -0.07261        0.979
            77          -0.07209        0.979
            78          -0.07158        0.980
            79          -0.07108        0.980
            80          -0.07059        0.980
            81          -0.07010        0.981
            82          -0.06963        0.982
            83          -0.06916        0.982
            84          -0.06870        0.982
            85          -0.06824        0.982
            86          -0.06779        0.982
            87          -0.06735        0.982
            88          -0.06692        0.983
            89          -0.06649        0.983
            90          -0.06607        0.984
            91          -0.06565        0.985
            92          -0.06524        0.985
            93          -0.06484        0.985
            94          -0.06444        0.986
            95          -0.06404        0.986
            96          -0.06366        0.986
            97          -0.06327        0.986
            98          -0.06290        0.986
            99          -0.06253        0.986
         Final          -0.06216        0.987
accuracy: 0.9453125
precision: 0.85
recall: 0.5396825396825397
F-measure: 0.6601941747572816
========Show top 10 most informative features========
   2.269 next_1_word_last_2_letters=='da' and label is 'class'
   1.980 prev_5_word_last_2_letters=='81' and label is 'class'
   1.768 prev_6_word_last_2_letters=='81' and label is 'class'
  -1.720 last_2_letters=='None' and label is 'class'
   1.566 last_2_letters=='la' and label is 'class'
   1.529 prev_7_word_last_2_letters=='to' and label is 'class'
  -1.480 word_pos=='DT' and label is 'class'
   1.420 prev_6_word_last_2_letters=='to' and label is 'class'
  -1.374 word_pos=='IN' and label is 'class'
   1.311 prev_3_word_last_2_letters=='lm' and label is 'class'
start [7] fold validation...
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -0.69315        0.094
             2          -0.18053        0.906
             3          -0.17722        0.906
             4          -0.17363        0.906
             5          -0.16989        0.906
             6          -0.16612        0.906
             7          -0.16240        0.906
             8          -0.15878        0.906
             9          -0.15529        0.906
            10          -0.15194        0.906
            11          -0.14875        0.906
            12          -0.14571        0.908
            13          -0.14281        0.910
            14          -0.14004        0.910
            15          -0.13740        0.916
            16          -0.13489        0.917
            17          -0.13248        0.919
            18          -0.13017        0.923
            19          -0.12796        0.927
            20          -0.12585        0.932
            21          -0.12381        0.934
            22          -0.12186        0.936
            23          -0.11998        0.936
            24          -0.11817        0.938
            25          -0.11643        0.940
            26          -0.11475        0.942
            27          -0.11313        0.945
            28          -0.11156        0.946
            29          -0.11005        0.948
            30          -0.10859        0.948
            31          -0.10717        0.948
            32          -0.10580        0.950
            33          -0.10447        0.952
            34          -0.10319        0.954
            35          -0.10194        0.953
            36          -0.10073        0.954
            37          -0.09956        0.955
            38          -0.09842        0.955
            39          -0.09731        0.958
            40          -0.09623        0.962
            41          -0.09519        0.963
            42          -0.09417        0.964
            43          -0.09318        0.964
            44          -0.09221        0.964
            45          -0.09127        0.965
            46          -0.09036        0.965
            47          -0.08947        0.965
            48          -0.08859        0.967
            49          -0.08775        0.967
            50          -0.08692        0.969
            51          -0.08611        0.970
            52          -0.08532        0.971
            53          -0.08455        0.971
            54          -0.08380        0.972
            55          -0.08306        0.974
            56          -0.08234        0.974
            57          -0.08164        0.974
            58          -0.08095        0.974
            59          -0.08027        0.975
            60          -0.07961        0.975
            61          -0.07897        0.975
            62          -0.07833        0.975
            63          -0.07771        0.975
            64          -0.07710        0.975
            65          -0.07651        0.975
            66          -0.07592        0.975
            67          -0.07535        0.975
            68          -0.07479        0.975
            69          -0.07423        0.975
            70          -0.07369        0.976
            71          -0.07316        0.976
            72          -0.07264        0.976
            73          -0.07213        0.976
            74          -0.07162        0.977
            75          -0.07113        0.977
            76          -0.07064        0.977
            77          -0.07016        0.977
            78          -0.06969        0.977
            79          -0.06923        0.977
            80          -0.06877        0.978
            81          -0.06832        0.978
            82          -0.06788        0.978
            83          -0.06745        0.978
            84          -0.06702        0.978
            85          -0.06660        0.978
            86          -0.06618        0.978
            87          -0.06578        0.978
            88          -0.06537        0.978
            89          -0.06498        0.979
            90          -0.06459        0.979
            91          -0.06420        0.980
            92          -0.06382        0.981
            93          -0.06345        0.981
            94          -0.06308        0.981
            95          -0.06272        0.981
            96          -0.06236        0.981
            97          -0.06200        0.981
            98          -0.06165        0.981
            99          -0.06131        0.981
         Final          -0.06097        0.981
accuracy: 0.9328125
precision: 0.8378378378378378
recall: 0.45588235294117646
F-measure: 0.5904761904761905
========Show top 10 most informative features========
  -2.224 last_2_letters=='None' and label is 'class'
   2.224 prev_5_word_last_2_letters=='81' and label is 'class'
   1.961 next_1_word_last_2_letters=='da' and label is 'class'
   1.685 prev_5_word_last_2_letters=='ep' and label is 'class'
   1.565 prev_7_word_last_2_letters=='to' and label is 'class'
   1.452 prev_1_word_last_2_letters=='la' and label is 'class'
   1.366 prev_7_word_last_2_letters=='ff' and label is 'class'
  -1.250 word_pos=='IN' and label is 'class'
   1.160 prev_4_word_last_2_letters=='HD' and label is 'class'
   1.109 prev_3_word_last_2_letters=='ic' and label is 'class'
start [8] fold validation...
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -0.69315        0.096
             2          -0.18469        0.904
             3          -0.18142        0.904
             4          -0.17785        0.904
             5          -0.17415        0.904
             6          -0.17042        0.904
             7          -0.16675        0.904
             8          -0.16319        0.904
             9          -0.15977        0.904
            10          -0.15650        0.904
            11          -0.15339        0.904
            12          -0.15042        0.904
            13          -0.14760        0.907
            14          -0.14492        0.912
            15          -0.14236        0.914
            16          -0.13993        0.918
            17          -0.13760        0.921
            18          -0.13537        0.924
            19          -0.13324        0.927
            20          -0.13119        0.928
            21          -0.12923        0.930
            22          -0.12734        0.931
            23          -0.12552        0.934
            24          -0.12376        0.936
            25          -0.12207        0.936
            26          -0.12044        0.938
            27          -0.11886        0.940
            28          -0.11733        0.943
            29          -0.11585        0.945
            30          -0.11442        0.946
            31          -0.11303        0.946
            32          -0.11168        0.947
            33          -0.11037        0.949
            34          -0.10910        0.950
            35          -0.10786        0.950
            36          -0.10666        0.950
            37          -0.10549        0.952
            38          -0.10435        0.954
            39          -0.10324        0.956
            40          -0.10216        0.956
            41          -0.10111        0.956
            42          -0.10009        0.957
            43          -0.09908        0.959
            44          -0.09811        0.959
            45          -0.09715        0.959
            46          -0.09622        0.960
            47          -0.09531        0.960
            48          -0.09442        0.962
            49          -0.09355        0.962
            50          -0.09270        0.963
            51          -0.09187        0.964
            52          -0.09106        0.965
            53          -0.09026        0.965
            54          -0.08948        0.965
            55          -0.08872        0.965
            56          -0.08797        0.965
            57          -0.08724        0.965
            58          -0.08652        0.965
            59          -0.08582        0.966
            60          -0.08513        0.968
            61          -0.08446        0.968
            62          -0.08379        0.968
            63          -0.08314        0.969
            64          -0.08250        0.969
            65          -0.08188        0.969
            66          -0.08126        0.969
            67          -0.08066        0.969
            68          -0.08006        0.969
            69          -0.07948        0.970
            70          -0.07891        0.970
            71          -0.07834        0.970
            72          -0.07779        0.970
            73          -0.07725        0.971
            74          -0.07671        0.971
            75          -0.07619        0.971
            76          -0.07567        0.973
            77          -0.07516        0.973
            78          -0.07466        0.974
            79          -0.07417        0.975
            80          -0.07368        0.975
            81          -0.07321        0.975
            82          -0.07274        0.977
            83          -0.07227        0.977
            84          -0.07182        0.978
            85          -0.07137        0.978
            86          -0.07093        0.979
            87          -0.07049        0.979
            88          -0.07006        0.979
            89          -0.06964        0.980
            90          -0.06922        0.980
            91          -0.06881        0.980
            92          -0.06840        0.981
            93          -0.06801        0.981
            94          -0.06761        0.982
            95          -0.06722        0.982
            96          -0.06684        0.982
            97          -0.06646        0.984
            98          -0.06609        0.984
            99          -0.06572        0.984
         Final          -0.06536        0.985
accuracy: 0.9359375
precision: 0.9285714285714286
recall: 0.4
F-measure: 0.5591397849462366
========Show top 10 most informative features========
  -2.330 last_2_letters=='None' and label is 'class'
   2.001 next_1_word_last_2_letters=='da' and label is 'class'
   1.728 prev_7_word_last_2_letters=='to' and label is 'class'
   1.377 prev_7_word_last_2_letters=='81' and label is 'class'
   1.306 prev_5_word_last_2_letters=='ep' and label is 'class'
   1.201 prev_6_word_last_2_letters=='50' and label is 'class'
   1.181 prev_6_word_last_2_letters=='to' and label is 'class'
   1.162 prev_2_word_last_2_letters=='ue' and label is 'class'
   1.085 last_2_letters=='ft' and label is 'class'
   1.072 prev_1_word_last_2_letters=='11' and label is 'class'
start [9] fold validation...
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -0.69315        0.105
             2          -0.19982        0.895
             3          -0.19594        0.895
             4          -0.19168        0.895
             5          -0.18729        0.895
             6          -0.18290        0.895
             7          -0.17863        0.895
             8          -0.17451        0.895
             9          -0.17058        0.895
            10          -0.16685        0.895
            11          -0.16332        0.896
            12          -0.15997        0.898
            13          -0.15681        0.905
            14          -0.15381        0.908
            15          -0.15096        0.913
            16          -0.14826        0.916
            17          -0.14568        0.920
            18          -0.14323        0.922
            19          -0.14089        0.924
            20          -0.13865        0.926
            21          -0.13650        0.928
            22          -0.13445        0.931
            23          -0.13247        0.932
            24          -0.13058        0.935
            25          -0.12875        0.936
            26          -0.12700        0.939
            27          -0.12530        0.940
            28          -0.12367        0.941
            29          -0.12209        0.942
            30          -0.12056        0.942
            31          -0.11908        0.944
            32          -0.11765        0.945
            33          -0.11627        0.946
            34          -0.11492        0.948
            35          -0.11362        0.950
            36          -0.11236        0.950
            37          -0.11113        0.950
            38          -0.10994        0.952
            39          -0.10878        0.952
            40          -0.10765        0.953
            41          -0.10655        0.953
            42          -0.10548        0.953
            43          -0.10444        0.956
            44          -0.10343        0.956
            45          -0.10244        0.958
            46          -0.10148        0.960
            47          -0.10054        0.960
            48          -0.09962        0.961
            49          -0.09872        0.962
            50          -0.09785        0.962
            51          -0.09699        0.963
            52          -0.09616        0.963
            53          -0.09534        0.964
            54          -0.09454        0.967
            55          -0.09376        0.967
            56          -0.09299        0.968
            57          -0.09224        0.968
            58          -0.09151        0.968
            59          -0.09079        0.969
            60          -0.09008        0.969
            61          -0.08939        0.969
            62          -0.08872        0.970
            63          -0.08805        0.971
            64          -0.08740        0.971
            65          -0.08676        0.971
            66          -0.08613        0.971
            67          -0.08552        0.972
            68          -0.08491        0.972
            69          -0.08432        0.973
            70          -0.08374        0.973
            71          -0.08317        0.974
            72          -0.08260        0.974
            73          -0.08205        0.975
            74          -0.08150        0.975
            75          -0.08097        0.975
            76          -0.08044        0.975
            77          -0.07993        0.975
            78          -0.07942        0.975
            79          -0.07892        0.976
            80          -0.07842        0.976
            81          -0.07794        0.977
            82          -0.07746        0.977
            83          -0.07699        0.979
            84          -0.07652        0.979
            85          -0.07606        0.979
            86          -0.07561        0.979
            87          -0.07517        0.979
            88          -0.07473        0.980
            89          -0.07430        0.980
            90          -0.07388        0.980
            91          -0.07346        0.980
            92          -0.07304        0.980
            93          -0.07263        0.980
            94          -0.07223        0.980
            95          -0.07183        0.980
            96          -0.07144        0.980
            97          -0.07106        0.981
            98          -0.07067        0.981
            99          -0.07030        0.981
         Final          -0.06992        0.981
accuracy: 0.959375
precision: 1.0
recall: 0.5
F-measure: 0.6666666666666666
========Show top 10 most informative features========
   2.161 prev_6_word_last_2_letters=='81' and label is 'class'
   2.159 prev_5_word_last_2_letters=='81' and label is 'class'
  -1.705 last_2_letters=='None' and label is 'class'
   1.516 prev_5_word_last_2_letters=='ep' and label is 'class'
   1.477 prev_3_word_last_2_letters=='ue' and label is 'class'
   1.477 prev_1_word_last_2_letters=='ft' and label is 'class'
  -1.450 word_pos=='IN' and label is 'class'
   1.380 prev_2_word_last_2_letters=='ue' and label is 'class'
  -1.340 word_pos=='DT' and label is 'class'
   1.315 next_1_word_last_2_letters=='ft' and label is 'class'
all_f_measure, [0.5591397849462365, 0.5773195876288659, 0.5494505494505495, 0.66, 0.5904761904761905, 0.6601941747572816, 0.5904761904761905, 0.5591397849462366, 0.6666666666666666]
all_precision, [0.7222222222222222, 0.7567567567567568, 0.8064516129032258, 0.8918918918918919, 0.7380952380952381, 0.85, 0.8378378378378378, 0.9285714285714286, 1.0]
all_recall [0.45614035087719296, 0.4666666666666667, 0.4166666666666667, 0.5238095238095238, 0.49206349206349204, 0.5396825396825397, 0.45588235294117646, 0.4, 0.5]
Final F-measure 0.6014292143720242
Final precision 0.836869665364289
Final recall 0.47232351030080644
