test Maximum Entropy classifer for class induction
training data is loaded!
gazetteers are loaded!
skip computing features...
load features from 'trainWithFeatures.json'... 
train set size 4268
 split  90.0 % from gold standards for training ... 
start [1] fold validation...
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -0.69315        0.099
             2          -0.20581        0.901
             3          -0.19862        0.901
             4          -0.19347        0.901
             5          -0.18837        0.901
             6          -0.18333        0.901
             7          -0.17838        0.901
             8          -0.17356        0.901
             9          -0.16889        0.901
            10          -0.16439        0.901
            11          -0.16008        0.901
            12          -0.15597        0.901
            13          -0.15205        0.904
            14          -0.14832        0.908
            15          -0.14478        0.910
            16          -0.14141        0.915
            17          -0.13822        0.921
            18          -0.13520        0.927
            19          -0.13232        0.935
            20          -0.12959        0.938
            21          -0.12700        0.941
            22          -0.12454        0.943
            23          -0.12219        0.944
            24          -0.11995        0.945
            25          -0.11782        0.947
            26          -0.11578        0.949
            27          -0.11383        0.950
            28          -0.11197        0.954
            29          -0.11018        0.956
            30          -0.10847        0.956
            31          -0.10683        0.957
            32          -0.10526        0.958
            33          -0.10374        0.959
            34          -0.10228        0.959
            35          -0.10088        0.960
            36          -0.09952        0.961
            37          -0.09821        0.961
            38          -0.09695        0.961
            39          -0.09573        0.961
            40          -0.09455        0.962
            41          -0.09341        0.962
            42          -0.09230        0.963
            43          -0.09123        0.964
            44          -0.09019        0.966
            45          -0.08918        0.966
            46          -0.08820        0.967
            47          -0.08725        0.968
            48          -0.08632        0.968
            49          -0.08542        0.970
            50          -0.08454        0.971
            51          -0.08369        0.971
            52          -0.08286        0.971
            53          -0.08205        0.971
            54          -0.08126        0.972
            55          -0.08048        0.972
            56          -0.07973        0.972
            57          -0.07900        0.972
            58          -0.07828        0.973
            59          -0.07757        0.973
            60          -0.07689        0.974
            61          -0.07622        0.975
            62          -0.07556        0.976
            63          -0.07492        0.977
            64          -0.07429        0.977
            65          -0.07367        0.977
            66          -0.07307        0.978
            67          -0.07247        0.978
            68          -0.07189        0.978
            69          -0.07132        0.978
            70          -0.07077        0.978
            71          -0.07022        0.978
            72          -0.06968        0.978
            73          -0.06915        0.979
            74          -0.06864        0.980
            75          -0.06813        0.980
            76          -0.06763        0.982
            77          -0.06714        0.982
            78          -0.06665        0.982
            79          -0.06618        0.982
            80          -0.06571        0.982
            81          -0.06525        0.982
            82          -0.06480        0.982
            83          -0.06436        0.983
            84          -0.06392        0.983
            85          -0.06349        0.983
            86          -0.06307        0.984
            87          -0.06265        0.984
            88          -0.06224        0.984
            89          -0.06184        0.984
            90          -0.06144        0.985
            91          -0.06105        0.985
            92          -0.06066        0.985
            93          -0.06028        0.985
            94          -0.05990        0.985
            95          -0.05953        0.986
            96          -0.05917        0.986
            97          -0.05881        0.986
            98          -0.05845        0.987
            99          -0.05810        0.987
         Final          -0.05776        0.987
accuracy: 0.9859484777517564
precision: 1.0
recall: 0.8333333333333334
F-measure: 0.9090909090909091
========Show top 10 most informative features========
   2.160 word=='artist' and label is 'class'
  -2.089 last_2_letters=='None' and label is 'class'
   2.011 prev_6_word_last_2_letters=='81' and label is 'class'
   1.904 prev_5_word_last_2_letters=='81' and label is 'class'
  -1.868 word_pos=='DT' and label is 'class'
   1.831 next_1_word_last_2_letters=='da' and label is 'class'
   1.709 prev_1_word_last_2_letters=='11' and label is 'class'
   1.561 next_1_word_last_2_letters=='x1' and label is 'class'
  -1.414 word_pos=='IN' and label is 'class'
   1.288 prev_8_word_last_2_letters=='to' and label is 'class'
start [2] fold validation...
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -0.69315        0.097
             2          -0.20164        0.903
             3          -0.19456        0.903
             4          -0.18957        0.903
             5          -0.18463        0.903
             6          -0.17975        0.903
             7          -0.17495        0.903
             8          -0.17027        0.903
             9          -0.16574        0.903
            10          -0.16138        0.903
            11          -0.15719        0.903
            12          -0.15319        0.903
            13          -0.14937        0.906
            14          -0.14574        0.909
            15          -0.14228        0.911
            16          -0.13901        0.917
            17          -0.13589        0.923
            18          -0.13294        0.928
            19          -0.13013        0.935
            20          -0.12746        0.938
            21          -0.12493        0.940
            22          -0.12252        0.943
            23          -0.12022        0.944
            24          -0.11803        0.946
            25          -0.11594        0.948
            26          -0.11395        0.949
            27          -0.11204        0.953
            28          -0.11022        0.954
            29          -0.10847        0.955
            30          -0.10680        0.956
            31          -0.10519        0.958
            32          -0.10365        0.958
            33          -0.10216        0.960
            34          -0.10073        0.962
            35          -0.09935        0.963
            36          -0.09803        0.964
            37          -0.09675        0.964
            38          -0.09551        0.965
            39          -0.09431        0.965
            40          -0.09316        0.965
            41          -0.09204        0.966
            42          -0.09096        0.966
            43          -0.08991        0.966
            44          -0.08889        0.966
            45          -0.08790        0.966
            46          -0.08694        0.966
            47          -0.08600        0.970
            48          -0.08510        0.970
            49          -0.08421        0.971
            50          -0.08335        0.971
            51          -0.08252        0.973
            52          -0.08170        0.973
            53          -0.08091        0.973
            54          -0.08013        0.973
            55          -0.07938        0.973
            56          -0.07864        0.974
            57          -0.07792        0.975
            58          -0.07721        0.975
            59          -0.07653        0.976
            60          -0.07585        0.976
            61          -0.07520        0.977
            62          -0.07455        0.977
            63          -0.07392        0.977
            64          -0.07330        0.978
            65          -0.07270        0.978
            66          -0.07211        0.978
            67          -0.07153        0.978
            68          -0.07096        0.978
            69          -0.07040        0.978
            70          -0.06985        0.978
            71          -0.06932        0.978
            72          -0.06879        0.978
            73          -0.06827        0.978
            74          -0.06776        0.979
            75          -0.06726        0.979
            76          -0.06677        0.979
            77          -0.06629        0.979
            78          -0.06582        0.981
            79          -0.06535        0.982
            80          -0.06489        0.983
            81          -0.06444        0.983
            82          -0.06400        0.983
            83          -0.06356        0.983
            84          -0.06314        0.983
            85          -0.06271        0.983
            86          -0.06230        0.983
            87          -0.06189        0.983
            88          -0.06148        0.983
            89          -0.06109        0.983
            90          -0.06070        0.983
            91          -0.06031        0.983
            92          -0.05993        0.984
            93          -0.05956        0.984
            94          -0.05919        0.985
            95          -0.05882        0.986
            96          -0.05846        0.986
            97          -0.05811        0.986
            98          -0.05776        0.986
            99          -0.05742        0.986
         Final          -0.05708        0.986
accuracy: 0.9695550351288056
precision: 0.9696969696969697
recall: 0.7272727272727273
F-measure: 0.8311688311688312
========Show top 10 most informative features========
  -2.071 last_2_letters=='None' and label is 'class'
   2.019 prev_6_word_last_2_letters=='81' and label is 'class'
  -1.888 word_pos=='DT' and label is 'class'
   1.851 prev_5_word_last_2_letters=='81' and label is 'class'
   1.787 next_1_word_last_2_letters=='da' and label is 'class'
   1.704 word=='artist' and label is 'class'
   1.673 prev_1_word_last_2_letters=='11' and label is 'class'
  -1.519 word_pos=='CD' and label is 'class'
   1.505 next_1_word_last_2_letters=='x1' and label is 'class'
  -1.426 word_pos=='IN' and label is 'class'
start [3] fold validation...
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -0.69315        0.099
             2          -0.20442        0.901
             3          -0.19725        0.901
             4          -0.19216        0.901
             5          -0.18711        0.901
             6          -0.18211        0.901
             7          -0.17721        0.901
             8          -0.17243        0.901
             9          -0.16781        0.901
            10          -0.16336        0.901
            11          -0.15910        0.901
            12          -0.15503        0.902
            13          -0.15115        0.905
            14          -0.14747        0.907
            15          -0.14397        0.912
            16          -0.14065        0.917
            17          -0.13750        0.923
            18          -0.13452        0.928
            19          -0.13168        0.936
            20          -0.12899        0.938
            21          -0.12644        0.940
            22          -0.12401        0.940
            23          -0.12169        0.942
            24          -0.11949        0.944
            25          -0.11739        0.946
            26          -0.11538        0.947
            27          -0.11346        0.949
            28          -0.11162        0.952
            29          -0.10987        0.955
            30          -0.10818        0.956
            31          -0.10657        0.958
            32          -0.10501        0.959
            33          -0.10352        0.960
            34          -0.10208        0.961
            35          -0.10070        0.961
            36          -0.09936        0.962
            37          -0.09807        0.963
            38          -0.09683        0.964
            39          -0.09563        0.964
            40          -0.09446        0.964
            41          -0.09334        0.964
            42          -0.09225        0.965
            43          -0.09119        0.965
            44          -0.09016        0.966
            45          -0.08917        0.966
            46          -0.08820        0.966
            47          -0.08726        0.967
            48          -0.08635        0.968
            49          -0.08546        0.968
            50          -0.08459        0.969
            51          -0.08375        0.970
            52          -0.08293        0.972
            53          -0.08213        0.973
            54          -0.08135        0.974
            55          -0.08058        0.974
            56          -0.07984        0.974
            57          -0.07911        0.975
            58          -0.07840        0.975
            59          -0.07771        0.976
            60          -0.07703        0.976
            61          -0.07637        0.977
            62          -0.07572        0.977
            63          -0.07508        0.977
            64          -0.07446        0.977
            65          -0.07385        0.978
            66          -0.07325        0.978
            67          -0.07266        0.978
            68          -0.07209        0.978
            69          -0.07152        0.978
            70          -0.07097        0.979
            71          -0.07043        0.979
            72          -0.06989        0.979
            73          -0.06937        0.979
            74          -0.06886        0.979
            75          -0.06835        0.980
            76          -0.06785        0.981
            77          -0.06737        0.981
            78          -0.06689        0.981
            79          -0.06642        0.981
            80          -0.06595        0.981
            81          -0.06550        0.982
            82          -0.06505        0.982
            83          -0.06461        0.982
            84          -0.06417        0.982
            85          -0.06374        0.982
            86          -0.06332        0.982
            87          -0.06291        0.982
            88          -0.06250        0.982
            89          -0.06210        0.982
            90          -0.06170        0.982
            91          -0.06131        0.982
            92          -0.06092        0.983
            93          -0.06054        0.983
            94          -0.06017        0.983
            95          -0.05980        0.984
            96          -0.05944        0.985
            97          -0.05908        0.985
            98          -0.05872        0.985
            99          -0.05837        0.985
         Final          -0.05803        0.985
accuracy: 0.9695550351288056
precision: 0.9310344827586207
recall: 0.7105263157894737
F-measure: 0.8059701492537312
========Show top 10 most informative features========
  -2.083 last_2_letters=='None' and label is 'class'
   1.973 prev_6_word_last_2_letters=='81' and label is 'class'
   1.886 prev_5_word_last_2_letters=='81' and label is 'class'
  -1.881 word_pos=='DT' and label is 'class'
   1.660 prev_1_word_last_2_letters=='11' and label is 'class'
   1.635 next_1_word_last_2_letters=='da' and label is 'class'
   1.613 word=='artist' and label is 'class'
   1.520 next_1_word_last_2_letters=='x1' and label is 'class'
  -1.403 word_pos=='IN' and label is 'class'
   1.324 word=='Music' and label is 'class'
start [4] fold validation...
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -0.69315        0.099
             2          -0.20337        0.901
             3          -0.19615        0.901
             4          -0.19097        0.901
             5          -0.18582        0.901
             6          -0.18073        0.901
             7          -0.17573        0.901
             8          -0.17087        0.901
             9          -0.16617        0.901
            10          -0.16166        0.901
            11          -0.15734        0.901
            12          -0.15322        0.902
            13          -0.14931        0.905
            14          -0.14560        0.909
            15          -0.14207        0.916
            16          -0.13874        0.920
            17          -0.13558        0.925
            18          -0.13258        0.934
            19          -0.12975        0.938
            20          -0.12705        0.940
            21          -0.12450        0.942
            22          -0.12207        0.945
            23          -0.11977        0.945
            24          -0.11757        0.946
            25          -0.11547        0.948
            26          -0.11348        0.951
            27          -0.11157        0.952
            28          -0.10975        0.955
            29          -0.10800        0.957
            30          -0.10633        0.957
            31          -0.10472        0.959
            32          -0.10318        0.959
            33          -0.10170        0.960
            34          -0.10028        0.960
            35          -0.09891        0.960
            36          -0.09758        0.960
            37          -0.09631        0.960
            38          -0.09508        0.961
            39          -0.09389        0.961
            40          -0.09274        0.961
            41          -0.09162        0.961
            42          -0.09055        0.962
            43          -0.08950        0.962
            44          -0.08849        0.963
            45          -0.08751        0.964
            46          -0.08655        0.966
            47          -0.08562        0.968
            48          -0.08472        0.970
            49          -0.08384        0.971
            50          -0.08299        0.971
            51          -0.08216        0.971
            52          -0.08135        0.972
            53          -0.08056        0.973
            54          -0.07979        0.974
            55          -0.07904        0.974
            56          -0.07830        0.975
            57          -0.07759        0.976
            58          -0.07689        0.976
            59          -0.07620        0.976
            60          -0.07553        0.976
            61          -0.07488        0.976
            62          -0.07424        0.978
            63          -0.07361        0.978
            64          -0.07300        0.979
            65          -0.07240        0.979
            66          -0.07181        0.979
            67          -0.07123        0.979
            68          -0.07067        0.979
            69          -0.07011        0.979
            70          -0.06957        0.979
            71          -0.06904        0.980
            72          -0.06851        0.980
            73          -0.06800        0.980
            74          -0.06749        0.981
            75          -0.06700        0.982
            76          -0.06651        0.982
            77          -0.06603        0.982
            78          -0.06556        0.982
            79          -0.06510        0.982
            80          -0.06464        0.982
            81          -0.06419        0.983
            82          -0.06375        0.983
            83          -0.06332        0.983
            84          -0.06289        0.983
            85          -0.06247        0.984
            86          -0.06206        0.984
            87          -0.06165        0.984
            88          -0.06125        0.985
            89          -0.06086        0.985
            90          -0.06047        0.985
            91          -0.06008        0.985
            92          -0.05971        0.985
            93          -0.05933        0.985
            94          -0.05897        0.985
            95          -0.05860        0.985
            96          -0.05825        0.985
            97          -0.05789        0.986
            98          -0.05755        0.986
            99          -0.05720        0.986
         Final          -0.05687        0.986
accuracy: 0.9789227166276346
precision: 1.0
recall: 0.7692307692307693
F-measure: 0.8695652173913044
========Show top 10 most informative features========
   2.400 prev_5_word_last_2_letters=='81' and label is 'class'
  -2.297 last_2_letters=='None' and label is 'class'
   1.993 prev_6_word_last_2_letters=='81' and label is 'class'
  -1.832 word_pos=='DT' and label is 'class'
  -1.810 word_pos=='IN' and label is 'class'
   1.808 next_1_word_last_2_letters=='da' and label is 'class'
   1.795 next_1_word_last_2_letters=='x1' and label is 'class'
   1.711 word=='artist' and label is 'class'
   1.629 prev_1_word_last_2_letters=='11' and label is 'class'
   1.393 prev_2_word_last_2_letters=='ue' and label is 'class'
start [5] fold validation...
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -0.69315        0.097
             2          -0.20051        0.903
             3          -0.19343        0.903
             4          -0.18849        0.903
             5          -0.18361        0.903
             6          -0.17878        0.903
             7          -0.17404        0.903
             8          -0.16941        0.903
             9          -0.16493        0.903
            10          -0.16061        0.903
            11          -0.15646        0.903
            12          -0.15249        0.904
            13          -0.14870        0.906
            14          -0.14509        0.907
            15          -0.14166        0.912
            16          -0.13839        0.919
            17          -0.13530        0.923
            18          -0.13235        0.927
            19          -0.12956        0.934
            20          -0.12690        0.938
            21          -0.12438        0.939
            22          -0.12197        0.940
            23          -0.11968        0.944
            24          -0.11750        0.946
            25          -0.11541        0.949
            26          -0.11343        0.950
            27          -0.11152        0.953
            28          -0.10970        0.954
            29          -0.10796        0.955
            30          -0.10629        0.956
            31          -0.10468        0.957
            32          -0.10314        0.959
            33          -0.10166        0.960
            34          -0.10023        0.961
            35          -0.09885        0.963
            36          -0.09753        0.964
            37          -0.09625        0.964
            38          -0.09501        0.964
            39          -0.09382        0.964
            40          -0.09266        0.965
            41          -0.09155        0.965
            42          -0.09046        0.966
            43          -0.08941        0.967
            44          -0.08839        0.967
            45          -0.08741        0.967
            46          -0.08645        0.968
            47          -0.08551        0.968
            48          -0.08461        0.969
            49          -0.08373        0.970
            50          -0.08287        0.971
            51          -0.08203        0.971
            52          -0.08122        0.972
            53          -0.08042        0.973
            54          -0.07965        0.974
            55          -0.07889        0.974
            56          -0.07815        0.974
            57          -0.07743        0.975
            58          -0.07673        0.976
            59          -0.07604        0.976
            60          -0.07537        0.976
            61          -0.07471        0.976
            62          -0.07407        0.977
            63          -0.07344        0.977
            64          -0.07282        0.978
            65          -0.07222        0.978
            66          -0.07162        0.978
            67          -0.07104        0.978
            68          -0.07047        0.978
            69          -0.06992        0.979
            70          -0.06937        0.979
            71          -0.06883        0.979
            72          -0.06831        0.979
            73          -0.06779        0.979
            74          -0.06728        0.979
            75          -0.06678        0.980
            76          -0.06629        0.981
            77          -0.06581        0.981
            78          -0.06533        0.981
            79          -0.06487        0.981
            80          -0.06441        0.981
            81          -0.06396        0.981
            82          -0.06352        0.981
            83          -0.06308        0.982
            84          -0.06265        0.982
            85          -0.06223        0.982
            86          -0.06181        0.982
            87          -0.06140        0.982
            88          -0.06100        0.982
            89          -0.06060        0.982
            90          -0.06021        0.982
            91          -0.05983        0.982
            92          -0.05945        0.983
            93          -0.05907        0.984
            94          -0.05870        0.984
            95          -0.05834        0.985
            96          -0.05798        0.986
            97          -0.05762        0.986
            98          -0.05727        0.986
            99          -0.05693        0.986
         Final          -0.05659        0.986
accuracy: 0.9789227166276346
precision: 0.9523809523809523
recall: 0.851063829787234
F-measure: 0.898876404494382
========Show top 10 most informative features========
  -2.085 last_2_letters=='None' and label is 'class'
   2.004 prev_1_word_last_2_letters=='11' and label is 'class'
   1.965 prev_6_word_last_2_letters=='81' and label is 'class'
  -1.875 word_pos=='DT' and label is 'class'
   1.874 prev_5_word_last_2_letters=='81' and label is 'class'
   1.669 next_1_word_last_2_letters=='da' and label is 'class'
   1.647 word=='artist' and label is 'class'
   1.527 next_1_word_last_2_letters=='x1' and label is 'class'
  -1.401 word_pos=='IN' and label is 'class'
   1.342 prev_8_word_last_2_letters=='to' and label is 'class'
start [6] fold validation...
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -0.69315        0.098
             2          -0.20221        0.902
             3          -0.19497        0.902
             4          -0.18981        0.902
             5          -0.18470        0.902
             6          -0.17964        0.902
             7          -0.17467        0.902
             8          -0.16983        0.902
             9          -0.16514        0.902
            10          -0.16063        0.902
            11          -0.15631        0.902
            12          -0.15218        0.903
            13          -0.14826        0.906
            14          -0.14453        0.910
            15          -0.14099        0.915
            16          -0.13763        0.920
            17          -0.13445        0.924
            18          -0.13144        0.932
            19          -0.12858        0.937
            20          -0.12586        0.939
            21          -0.12329        0.939
            22          -0.12084        0.944
            23          -0.11851        0.945
            24          -0.11629        0.945
            25          -0.11417        0.947
            26          -0.11215        0.951
            27          -0.11022        0.953
            28          -0.10838        0.956
            29          -0.10662        0.958
            30          -0.10492        0.959
            31          -0.10330        0.961
            32          -0.10175        0.962
            33          -0.10025        0.962
            34          -0.09881        0.964
            35          -0.09742        0.965
            36          -0.09609        0.965
            37          -0.09480        0.964
            38          -0.09356        0.965
            39          -0.09236        0.965
            40          -0.09120        0.965
            41          -0.09007        0.966
            42          -0.08899        0.967
            43          -0.08793        0.967
            44          -0.08691        0.968
            45          -0.08592        0.968
            46          -0.08496        0.968
            47          -0.08403        0.969
            48          -0.08312        0.970
            49          -0.08224        0.971
            50          -0.08138        0.972
            51          -0.08055        0.972
            52          -0.07973        0.972
            53          -0.07894        0.974
            54          -0.07817        0.974
            55          -0.07741        0.975
            56          -0.07668        0.976
            57          -0.07596        0.976
            58          -0.07526        0.976
            59          -0.07458        0.976
            60          -0.07391        0.976
            61          -0.07325        0.977
            62          -0.07261        0.978
            63          -0.07199        0.978
            64          -0.07138        0.979
            65          -0.07078        0.980
            66          -0.07019        0.982
            67          -0.06961        0.983
            68          -0.06905        0.983
            69          -0.06850        0.983
            70          -0.06796        0.983
            71          -0.06743        0.983
            72          -0.06690        0.983
            73          -0.06639        0.983
            74          -0.06589        0.983
            75          -0.06540        0.983
            76          -0.06491        0.983
            77          -0.06444        0.983
            78          -0.06397        0.983
            79          -0.06351        0.983
            80          -0.06306        0.984
            81          -0.06262        0.984
            82          -0.06218        0.984
            83          -0.06175        0.985
            84          -0.06133        0.985
            85          -0.06091        0.985
            86          -0.06050        0.985
            87          -0.06010        0.985
            88          -0.05970        0.985
            89          -0.05931        0.985
            90          -0.05893        0.986
            91          -0.05855        0.986
            92          -0.05817        0.986
            93          -0.05781        0.986
            94          -0.05744        0.986
            95          -0.05709        0.986
            96          -0.05673        0.986
            97          -0.05639        0.986
            98          -0.05604        0.986
            99          -0.05571        0.987
         Final          -0.05537        0.987
accuracy: 0.9695550351288056
precision: 0.967741935483871
recall: 0.7142857142857143
F-measure: 0.8219178082191781
========Show top 10 most informative features========
   2.378 prev_5_word_last_2_letters=='81' and label is 'class'
  -2.289 last_2_letters=='None' and label is 'class'
   1.981 prev_6_word_last_2_letters=='81' and label is 'class'
  -1.875 word_pos=='DT' and label is 'class'
  -1.814 word_pos=='IN' and label is 'class'
   1.747 next_1_word_last_2_letters=='x1' and label is 'class'
   1.723 next_1_word_last_2_letters=='da' and label is 'class'
   1.684 prev_1_word_last_2_letters=='11' and label is 'class'
   1.609 word=='artist' and label is 'class'
   1.561 prev_5_word_last_2_letters=='ep' and label is 'class'
start [7] fold validation...
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -0.69315        0.097
             2          -0.19970        0.903
             3          -0.19272        0.903
             4          -0.18779        0.903
             5          -0.18289        0.903
             6          -0.17803        0.903
             7          -0.17326        0.903
             8          -0.16860        0.903
             9          -0.16408        0.903
            10          -0.15973        0.903
            11          -0.15556        0.903
            12          -0.15157        0.903
            13          -0.14776        0.905
            14          -0.14414        0.907
            15          -0.14070        0.914
            16          -0.13743        0.919
            17          -0.13433        0.923
            18          -0.13139        0.929
            19          -0.12859        0.935
            20          -0.12594        0.940
            21          -0.12341        0.943
            22          -0.12101        0.944
            23          -0.11873        0.945
            24          -0.11655        0.947
            25          -0.11447        0.948
            26          -0.11248        0.951
            27          -0.11059        0.955
            28          -0.10877        0.956
            29          -0.10703        0.958
            30          -0.10537        0.958
            31          -0.10377        0.960
            32          -0.10223        0.960
            33          -0.10075        0.962
            34          -0.09933        0.962
            35          -0.09796        0.963
            36          -0.09664        0.963
            37          -0.09537        0.963
            38          -0.09414        0.964
            39          -0.09295        0.964
            40          -0.09180        0.964
            41          -0.09068        0.965
            42          -0.08961        0.966
            43          -0.08856        0.966
            44          -0.08755        0.967
            45          -0.08656        0.968
            46          -0.08561        0.969
            47          -0.08468        0.969
            48          -0.08378        0.969
            49          -0.08290        0.970
            50          -0.08205        0.970
            51          -0.08121        0.971
            52          -0.08040        0.971
            53          -0.07961        0.972
            54          -0.07884        0.972
            55          -0.07809        0.973
            56          -0.07735        0.974
            57          -0.07664        0.974
            58          -0.07594        0.976
            59          -0.07525        0.976
            60          -0.07458        0.977
            61          -0.07393        0.978
            62          -0.07329        0.978
            63          -0.07266        0.978
            64          -0.07205        0.978
            65          -0.07144        0.978
            66          -0.07086        0.978
            67          -0.07028        0.978
            68          -0.06971        0.978
            69          -0.06916        0.979
            70          -0.06861        0.979
            71          -0.06808        0.980
            72          -0.06755        0.980
            73          -0.06704        0.980
            74          -0.06653        0.980
            75          -0.06604        0.981
            76          -0.06555        0.981
            77          -0.06507        0.982
            78          -0.06460        0.982
            79          -0.06414        0.982
            80          -0.06368        0.982
            81          -0.06323        0.982
            82          -0.06279        0.982
            83          -0.06236        0.983
            84          -0.06193        0.984
            85          -0.06152        0.984
            86          -0.06110        0.984
            87          -0.06070        0.984
            88          -0.06030        0.984
            89          -0.05990        0.984
            90          -0.05951        0.984
            91          -0.05913        0.984
            92          -0.05875        0.984
            93          -0.05838        0.984
            94          -0.05801        0.985
            95          -0.05765        0.985
            96          -0.05730        0.985
            97          -0.05695        0.985
            98          -0.05660        0.985
            99          -0.05626        0.985
         Final          -0.05592        0.986
accuracy: 0.9765807962529274
precision: 1.0
recall: 0.782608695652174
F-measure: 0.8780487804878049
========Show top 10 most informative features========
   2.712 prev_6_word_last_2_letters=='81' and label is 'class'
   2.437 prev_5_word_last_2_letters=='81' and label is 'class'
  -2.389 word_pos=='DT' and label is 'class'
  -2.287 last_2_letters=='None' and label is 'class'
   1.956 prev_1_word_last_2_letters=='11' and label is 'class'
  -1.824 word_pos=='IN' and label is 'class'
   1.754 next_1_word_last_2_letters=='x1' and label is 'class'
   1.690 next_1_word_last_2_letters=='da' and label is 'class'
   1.520 word=='artist' and label is 'class'
  -1.457 last_2_letters=='he' and label is 'class'
start [8] fold validation...
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -0.69315        0.099
             2          -0.20406        0.901
             3          -0.19692        0.901
             4          -0.19181        0.901
             5          -0.18673        0.901
             6          -0.18171        0.901
             7          -0.17677        0.901
             8          -0.17196        0.901
             9          -0.16731        0.901
            10          -0.16283        0.901
            11          -0.15853        0.901
            12          -0.15442        0.902
            13          -0.15051        0.904
            14          -0.14680        0.906
            15          -0.14326        0.914
            16          -0.13991        0.918
            17          -0.13673        0.923
            18          -0.13371        0.928
            19          -0.13085        0.935
            20          -0.12813        0.941
            21          -0.12554        0.942
            22          -0.12308        0.943
            23          -0.12074        0.944
            24          -0.11851        0.946
            25          -0.11638        0.949
            26          -0.11434        0.951
            27          -0.11240        0.954
            28          -0.11054        0.954
            29          -0.10876        0.954
            30          -0.10706        0.956
            31          -0.10542        0.957
            32          -0.10384        0.959
            33          -0.10233        0.959
            34          -0.10087        0.959
            35          -0.09947        0.960
            36          -0.09812        0.960
            37          -0.09681        0.960
            38          -0.09555        0.961
            39          -0.09433        0.961
            40          -0.09315        0.961
            41          -0.09201        0.962
            42          -0.09090        0.963
            43          -0.08983        0.964
            44          -0.08879        0.964
            45          -0.08778        0.965
            46          -0.08680        0.966
            47          -0.08585        0.966
            48          -0.08493        0.969
            49          -0.08403        0.972
            50          -0.08315        0.972
            51          -0.08230        0.973
            52          -0.08146        0.974
            53          -0.08065        0.975
            54          -0.07986        0.975
            55          -0.07909        0.975
            56          -0.07834        0.976
            57          -0.07760        0.977
            58          -0.07688        0.978
            59          -0.07618        0.978
            60          -0.07549        0.978
            61          -0.07482        0.978
            62          -0.07417        0.978
            63          -0.07352        0.978
            64          -0.07289        0.979
            65          -0.07228        0.979
            66          -0.07167        0.979
            67          -0.07108        0.979
            68          -0.07050        0.979
            69          -0.06993        0.979
            70          -0.06937        0.979
            71          -0.06883        0.979
            72          -0.06829        0.980
            73          -0.06776        0.980
            74          -0.06725        0.980
            75          -0.06674        0.980
            76          -0.06624        0.980
            77          -0.06575        0.980
            78          -0.06527        0.981
            79          -0.06479        0.982
            80          -0.06433        0.982
            81          -0.06387        0.983
            82          -0.06342        0.983
            83          -0.06297        0.984
            84          -0.06254        0.984
            85          -0.06211        0.984
            86          -0.06169        0.984
            87          -0.06127        0.984
            88          -0.06086        0.984
            89          -0.06046        0.984
            90          -0.06006        0.985
            91          -0.05967        0.985
            92          -0.05929        0.985
            93          -0.05891        0.986
            94          -0.05853        0.986
            95          -0.05816        0.986
            96          -0.05780        0.987
            97          -0.05744        0.987
            98          -0.05709        0.987
            99          -0.05674        0.987
         Final          -0.05639        0.987
accuracy: 0.9789227166276346
precision: 0.967741935483871
recall: 0.7894736842105263
F-measure: 0.8695652173913044
========Show top 10 most informative features========
   2.653 prev_6_word_last_2_letters=='81' and label is 'class'
  -2.431 word_pos=='DT' and label is 'class'
  -2.102 last_2_letters=='None' and label is 'class'
   1.958 prev_1_word_last_2_letters=='11' and label is 'class'
   1.862 prev_5_word_last_2_letters=='81' and label is 'class'
   1.668 next_1_word_last_2_letters=='da' and label is 'class'
   1.584 prev_5_word_last_2_letters=='ep' and label is 'class'
   1.584 word=='artist' and label is 'class'
   1.468 next_1_word_last_2_letters=='x1' and label is 'class'
  -1.460 last_2_letters=='he' and label is 'class'
start [9] fold validation...
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -0.69315        0.095
             2          -0.19708        0.905
             3          -0.18998        0.905
             4          -0.18508        0.905
             5          -0.18024        0.905
             6          -0.17546        0.905
             7          -0.17076        0.905
             8          -0.16619        0.905
             9          -0.16176        0.905
            10          -0.15749        0.905
            11          -0.15340        0.905
            12          -0.14949        0.906
            13          -0.14576        0.907
            14          -0.14222        0.911
            15          -0.13885        0.914
            16          -0.13565        0.921
            17          -0.13261        0.926
            18          -0.12973        0.930
            19          -0.12699        0.936
            20          -0.12439        0.941
            21          -0.12192        0.943
            22          -0.11957        0.944
            23          -0.11733        0.946
            24          -0.11520        0.946
            25          -0.11317        0.947
            26          -0.11123        0.948
            27          -0.10937        0.950
            28          -0.10759        0.952
            29          -0.10589        0.954
            30          -0.10426        0.954
            31          -0.10270        0.957
            32          -0.10119        0.958
            33          -0.09975        0.958
            34          -0.09836        0.958
            35          -0.09702        0.961
            36          -0.09572        0.961
            37          -0.09448        0.962
            38          -0.09327        0.963
            39          -0.09211        0.964
            40          -0.09098        0.964
            41          -0.08989        0.965
            42          -0.08884        0.965
            43          -0.08782        0.965
            44          -0.08682        0.966
            45          -0.08586        0.967
            46          -0.08492        0.969
            47          -0.08401        0.969
            48          -0.08313        0.970
            49          -0.08227        0.970
            50          -0.08143        0.972
            51          -0.08062        0.972
            52          -0.07982        0.973
            53          -0.07905        0.973
            54          -0.07829        0.973
            55          -0.07755        0.974
            56          -0.07683        0.974
            57          -0.07613        0.975
            58          -0.07544        0.975
            59          -0.07477        0.975
            60          -0.07411        0.975
            61          -0.07347        0.975
            62          -0.07284        0.976
            63          -0.07222        0.977
            64          -0.07162        0.977
            65          -0.07102        0.977
            66          -0.07044        0.978
            67          -0.06988        0.978
            68          -0.06932        0.978
            69          -0.06877        0.978
            70          -0.06824        0.978
            71          -0.06771        0.978
            72          -0.06719        0.979
            73          -0.06669        0.979
            74          -0.06619        0.979
            75          -0.06570        0.979
            76          -0.06522        0.979
            77          -0.06475        0.979
            78          -0.06428        0.980
            79          -0.06382        0.980
            80          -0.06337        0.981
            81          -0.06293        0.981
            82          -0.06250        0.981
            83          -0.06207        0.981
            84          -0.06165        0.981
            85          -0.06123        0.982
            86          -0.06082        0.982
            87          -0.06042        0.982
            88          -0.06002        0.982
            89          -0.05963        0.982
            90          -0.05925        0.982
            91          -0.05887        0.983
            92          -0.05850        0.983
            93          -0.05813        0.984
            94          -0.05776        0.984
            95          -0.05741        0.985
            96          -0.05705        0.985
            97          -0.05670        0.985
            98          -0.05636        0.985
            99          -0.05602        0.985
         Final          -0.05569        0.985
accuracy: 0.9672131147540983
precision: 1.0
recall: 0.7407407407407407
F-measure: 0.851063829787234
========Show top 10 most informative features========
   2.442 prev_5_word_last_2_letters=='81' and label is 'class'
  -2.304 last_2_letters=='None' and label is 'class'
   1.969 prev_6_word_last_2_letters=='81' and label is 'class'
  -1.864 word_pos=='DT' and label is 'class'
  -1.825 word_pos=='IN' and label is 'class'
   1.766 next_1_word_last_2_letters=='x1' and label is 'class'
   1.749 next_1_word_last_2_letters=='da' and label is 'class'
   1.655 prev_1_word_last_2_letters=='11' and label is 'class'
   1.554 word=='artist' and label is 'class'
   1.397 prev_2_word_last_2_letters=='ue' and label is 'class'
start [10] fold validation...
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -0.69315        0.100
             2          -0.20679        0.900
             3          -0.19953        0.900
             4          -0.19426        0.900
             5          -0.18903        0.900
             6          -0.18385        0.900
             7          -0.17877        0.900
             8          -0.17382        0.900
             9          -0.16903        0.900
            10          -0.16443        0.900
            11          -0.16002        0.900
            12          -0.15582        0.901
            13          -0.15183        0.903
            14          -0.14803        0.908
            15          -0.14443        0.912
            16          -0.14102        0.917
            17          -0.13778        0.924
            18          -0.13471        0.928
            19          -0.13180        0.936
            20          -0.12905        0.938
            21          -0.12642        0.941
            22          -0.12393        0.942
            23          -0.12156        0.943
            24          -0.11931        0.946
            25          -0.11716        0.946
            26          -0.11511        0.949
            27          -0.11314        0.951
            28          -0.11127        0.953
            29          -0.10948        0.956
            30          -0.10776        0.957
            31          -0.10611        0.958
            32          -0.10452        0.960
            33          -0.10300        0.960
            34          -0.10153        0.961
            35          -0.10012        0.961
            36          -0.09876        0.962
            37          -0.09745        0.964
            38          -0.09619        0.964
            39          -0.09496        0.964
            40          -0.09378        0.964
            41          -0.09264        0.963
            42          -0.09153        0.964
            43          -0.09046        0.964
            44          -0.08941        0.966
            45          -0.08840        0.967
            46          -0.08742        0.967
            47          -0.08647        0.970
            48          -0.08554        0.970
            49          -0.08464        0.971
            50          -0.08377        0.972
            51          -0.08291        0.972
            52          -0.08208        0.973
            53          -0.08127        0.974
            54          -0.08048        0.974
            55          -0.07971        0.975
            56          -0.07896        0.975
            57          -0.07822        0.976
            58          -0.07751        0.976
            59          -0.07680        0.976
            60          -0.07612        0.976
            61          -0.07545        0.977
            62          -0.07479        0.977
            63          -0.07415        0.977
            64          -0.07352        0.977
            65          -0.07291        0.978
            66          -0.07231        0.978
            67          -0.07172        0.978
            68          -0.07114        0.978
            69          -0.07057        0.979
            70          -0.07001        0.979
            71          -0.06947        0.979
            72          -0.06893        0.979
            73          -0.06841        0.980
            74          -0.06789        0.981
            75          -0.06738        0.981
            76          -0.06689        0.981
            77          -0.06640        0.981
            78          -0.06592        0.981
            79          -0.06544        0.981
            80          -0.06498        0.982
            81          -0.06452        0.982
            82          -0.06407        0.982
            83          -0.06363        0.982
            84          -0.06320        0.983
            85          -0.06277        0.983
            86          -0.06235        0.983
            87          -0.06193        0.984
            88          -0.06152        0.984
            89          -0.06112        0.984
            90          -0.06073        0.984
            91          -0.06034        0.984
            92          -0.05995        0.985
            93          -0.05957        0.985
            94          -0.05920        0.986
            95          -0.05883        0.986
            96          -0.05847        0.986
            97          -0.05811        0.986
            98          -0.05776        0.986
            99          -0.05741        0.986
         Final          -0.05707        0.987
accuracy: 0.9789227166276346
precision: 1.0
recall: 0.71875
F-measure: 0.8363636363636364
========Show top 10 most informative features========
   2.390 prev_5_word_last_2_letters=='81' and label is 'class'
  -2.279 last_2_letters=='None' and label is 'class'
   2.201 word=='artist' and label is 'class'
   1.973 prev_6_word_last_2_letters=='81' and label is 'class'
  -1.857 word_pos=='DT' and label is 'class'
  -1.823 word_pos=='IN' and label is 'class'
   1.772 next_1_word_last_2_letters=='da' and label is 'class'
   1.677 prev_8_word_last_2_letters=='to' and label is 'class'
   1.677 word=='Music' and label is 'class'
   1.656 prev_1_word_last_2_letters=='11' and label is 'class'
all_f_measure, [0.9090909090909091, 0.8311688311688312, 0.8059701492537312, 0.8695652173913044, 0.898876404494382, 0.8219178082191781, 0.8780487804878049, 0.8695652173913044, 0.851063829787234, 0.8363636363636364]
all_precision, [1.0, 0.9696969696969697, 0.9310344827586207, 1.0, 0.9523809523809523, 0.967741935483871, 1.0, 0.967741935483871, 1.0, 1.0]
all_recall [0.8333333333333334, 0.7272727272727273, 0.7105263157894737, 0.7692307692307693, 0.851063829787234, 0.7142857142857143, 0.782608695652174, 0.7894736842105263, 0.7407407407407407, 0.71875]
Final F-measure 0.8571630783648316
Final precision 0.9788596275804284
Final recall 0.7637285810302692
