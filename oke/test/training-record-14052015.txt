test Maximum Entropy classifer for class induction
training data is loaded!
gazetteers are loaded!
skip computing features...
load features from 'trainWithFeatures.json'... 
train set size 4268
 split  70.0 % from gold standards for training ... 
start [1] fold validation...
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -0.69315        0.098
             2          -0.20005        0.902
             3          -0.19298        0.902
             4          -0.18788        0.902
             5          -0.18280        0.902
             6          -0.17776        0.902
             7          -0.17280        0.902
             8          -0.16796        0.902
             9          -0.16326        0.902
            10          -0.15874        0.902
            11          -0.15441        0.902
            12          -0.15027        0.904
            13          -0.14633        0.907
            14          -0.14259        0.912
            15          -0.13903        0.917
            16          -0.13566        0.920
            17          -0.13246        0.927
            18          -0.12943        0.934
            19          -0.12655        0.939
            20          -0.12382        0.941
            21          -0.12122        0.943
            22          -0.11875        0.947
            23          -0.11641        0.947
            24          -0.11417        0.949
            25          -0.11204        0.949
            26          -0.11000        0.953
            27          -0.10806        0.954
            28          -0.10620        0.956
            29          -0.10442        0.957
            30          -0.10271        0.960
            31          -0.10108        0.960
            32          -0.09951        0.961
            33          -0.09800        0.963
            34          -0.09654        0.963
            35          -0.09515        0.965
            36          -0.09380        0.965
            37          -0.09250        0.966
            38          -0.09124        0.968
            39          -0.09003        0.969
            40          -0.08886        0.969
            41          -0.08773        0.969
            42          -0.08663        0.972
            43          -0.08557        0.972
            44          -0.08454        0.972
            45          -0.08354        0.973
            46          -0.08257        0.973
            47          -0.08163        0.973
            48          -0.08071        0.974
            49          -0.07982        0.976
            50          -0.07896        0.976
            51          -0.07811        0.976
            52          -0.07729        0.976
            53          -0.07649        0.976
            54          -0.07571        0.977
            55          -0.07495        0.977
            56          -0.07421        0.977
            57          -0.07349        0.978
            58          -0.07279        0.979
            59          -0.07210        0.979
            60          -0.07142        0.979
            61          -0.07076        0.979
            62          -0.07012        0.980
            63          -0.06949        0.980
            64          -0.06887        0.980
            65          -0.06827        0.980
            66          -0.06768        0.980
            67          -0.06710        0.981
            68          -0.06654        0.981
            69          -0.06598        0.981
            70          -0.06544        0.982
            71          -0.06490        0.984
            72          -0.06438        0.984
            73          -0.06387        0.984
            74          -0.06336        0.984
            75          -0.06287        0.985
            76          -0.06238        0.985
            77          -0.06191        0.985
            78          -0.06144        0.986
            79          -0.06098        0.986
            80          -0.06053        0.986
            81          -0.06008        0.986
            82          -0.05964        0.986
            83          -0.05921        0.987
            84          -0.05879        0.987
            85          -0.05838        0.987
            86          -0.05797        0.987
            87          -0.05757        0.987
            88          -0.05717        0.987
            89          -0.05678        0.987
            90          -0.05640        0.987
            91          -0.05602        0.987
            92          -0.05565        0.987
            93          -0.05528        0.987
            94          -0.05492        0.988
            95          -0.05456        0.988
            96          -0.05421        0.988
            97          -0.05387        0.988
            98          -0.05353        0.988
            99          -0.05319        0.988
         Final          -0.05286        0.988
accuracy: 0.965625
precision: 0.9659090909090909
recall: 0.6746031746031746
F-measure: 0.794392523364486
========Show top 10 most informative features========
  -2.409 last_2_letters=='None' and label is 'class'
   2.131 next_1_word_last_2_letters=='da' and label is 'class'
   2.111 prev_5_word_last_2_letters=='81' and label is 'class'
   2.083 word=='artist' and label is 'class'
   1.770 prev_6_word_last_2_letters=='81' and label is 'class'
  -1.643 word_pos=='DT' and label is 'class'
   1.594 prev_5_word_last_2_letters=='ep' and label is 'class'
  -1.568 word_pos=='IN' and label is 'class'
   1.495 prev_1_word_last_2_letters=='11' and label is 'class'
   1.329 prev_6_word_last_2_letters=='ec' and label is 'class'
all_f_measure, [0.794392523364486]
all_precision, [0.9659090909090909]
all_recall [0.6746031746031746]
Final F-measure 0.794392523364486
Final precision 0.9659090909090909
Final recall 0.6746031746031746
